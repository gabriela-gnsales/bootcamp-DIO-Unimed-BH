{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FnwfihrXHnp7"
      },
      "outputs": [],
      "source": [
        "# Importando as bibliotecas\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[THE MNIST DATABASE](http://yann.lecun.com/exdb/mnist/)**"
      ],
      "metadata": {
        "id": "kK4JXoBENBao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download de um dataset de imagens\n",
        "# imagem em tensor -> tem um padrão relacionado numérico que representa essa imagem em um formato que as redes de deeplearning que rodam em tensor flow estão acostumadas a trabalhar\n",
        "\n",
        "transform = transforms.ToTensor() # definindo a conversão de imagem para tensor\n",
        "\n",
        "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) # carrega a parte de treino do dataset\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # cria um buffer para pegar os dados por partes\n",
        "\n",
        "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform) # carrega a parte de validação do dataset\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True) # cria um buffer para pegar os dados por partes"
      ],
      "metadata": {
        "id": "bkQMJWWvITT6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando uma imagem da base de treinamento para conferir se a leitura dos dados está correta\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "imagens, etiquetas = dataiter.next()\n",
        "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "yoZKjPhaJoOQ",
        "outputId": "57ca8f5c-55af-4689-b307-b1bcefa9c002"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "3 # Conferindo o tamanho de uma imagem para verificar o tamanho do tensor que representa a imagem\n",
        "\n",
        "print(imagens[0].shape) # para verificar as dimensões do tensor de cada imagem\n",
        "print(etiquetas[0].shape) # para verificar as dimensões do tensor de cada etiqueta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkd-DWieKzgS",
        "outputId": "134b6c75-b2b5-4ad8-9a9d-fc8b914d7b13"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[Modelos de redes neurais](https://keras.io/api/applications/) -> InceptionV3**"
      ],
      "metadata": {
        "id": "EvRIllgBMa5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelo(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Modelo, self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28, 128) # camada de entrada, 784 neurônios que se ligam a 128\n",
        "    self.linear2 = nn.Linear(128, 64) # camada interna 1, 128 neurônios que se ligam a 64\n",
        "    self.linear3 = nn.Linear(64, 10) # camada interna 2, 64 neurônios que se ligam a 10\n",
        "    # para a camada de saída não é necessário definir nada, pois só precisamos pegar o output da camada interna 2\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = F.relu(self.linear1(X)) # função de ativação da camada de entrada para a camada interna 1\n",
        "    X = F.relu(self.linear2(X)) # função de ativação da camada interna 1 para a camada interna 2\n",
        "    X = self.linear3(X) # função de ativação da camada interna 2 para a camada de saída, nesse caso f(x) = x\n",
        "    return F.log_softmax(X, dim=1) # dados utilizados para calcular a perda"
      ],
      "metadata": {
        "id": "tadvwl9pMD0E"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento da rede\n",
        "\n",
        "def treino(modelo, trainloader, device):\n",
        "\n",
        "  otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5) # defini a política de atualização dos pesos e da bias\n",
        "  inicio = time() # timer para sabermos quanto tempo levou o treino\n",
        "\n",
        "  criterio = nn.NLLLoss() # definindo o critério para calcular a perda\n",
        "  EPOCHS = 10 # número de epochs que o algoritmo rodará\n",
        "  modelo.train() # ativando o modo de treinamento do modelo\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    perda_acumulada = 0 # inicialização da perda acumulada da epoch em questão\n",
        "\n",
        "    for imagens, etiquetas in trainloader:\n",
        "\n",
        "      imagens = imagens.view(imagens.shape[0], -1) # convertendo as imagens para \"vetores\" de 28*28 casas para ficarem compatíveis com a \n",
        "      otimizador.zero_grad() # zerando os gradinetes por conta do ciclo anterior\n",
        "\n",
        "      output = modelo(imagens.to(device)) # colocando os dados no modelo\n",
        "      perda_instantanea = criterio(output, etiquetas.to(device)) # calculando a perda da epoch em questão\n",
        "\n",
        "      perda_instantanea.backward() # back propagation a partir da perda\n",
        "\n",
        "      otimizador.step() # atualizando os pesos e a bias\n",
        "\n",
        "      perda_acumulada += perda_instantanea.item() # atualização da perda acumulada\n",
        "\n",
        "    else:\n",
        "      print(f'Epoch {epoch+1} - Perda resultante: {perda_acumulada/len(trainloader)}')\n",
        "    print('\\nTempo de treino (em minutos) =', (time()-inicio)/60)\n"
      ],
      "metadata": {
        "id": "jk_57q3PPlLI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rodando a validação\n",
        "\n",
        "def validacao(modelo, valloader, device):\n",
        "  conta_corretas, conta_todas = 0, 0\n",
        "  for imagens, etiquetas in valloader:\n",
        "    for i in range(len(etiquetas)):\n",
        "      img = imagens[i].view(1, 784)\n",
        "      # desativar o autograd para acelerar a validação\n",
        "      # grafos computacionais dinâmicos tem um custo alto de processamento\n",
        "      with torch.no_grad():\n",
        "        logps = modelo(img.to(device)) # output do modelo em escala logarítmica\n",
        "\n",
        "      ps = torch.exp(logps) # converte output para escala normal (lembrando que é um tensor)\n",
        "      probab = list(ps.cpu().numpy()[0])\n",
        "      etiqueta_pred = probab.index(max(probab)) # converte o tensor em um número, no caso, o número que o modelo previu\n",
        "      etiqueta_certa = etiquetas.numpy()[i]\n",
        "      if(etiqueta_certa == etiqueta_pred): # compra a previsão com o valor correto\n",
        "        conta_corretas += 1\n",
        "      conta_todas += 1\n",
        "\n",
        "    print('Total de imagens testadas =', conta_todas)\n",
        "    print(f'\\nPrevisão do modelo = {conta_corretas*100/conta_todas}')"
      ],
      "metadata": {
        "id": "g5NQeWA3Sgvw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lendo o modelo\n",
        "\n",
        "modelo = Modelo()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "modelo.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDOxKSxnVMEG",
        "outputId": "5f9fe028-bcdb-406e-ef66-46b4c4379619"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Modelo(\n",
              "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}